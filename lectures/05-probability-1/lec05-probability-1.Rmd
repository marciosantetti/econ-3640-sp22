---
title: ".b[Probability Theory, pt. I]"
subtitle: ".b[ECON 3640--001]"
author: "Marcio Santetti"
date: "Spring 2022"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'utah-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, ggforce, viridis, dplyr, magrittr, knitr, parallel, xaringanExtra, tidyverse, sjPlot, showtext, mathjaxr, ggforce, furrr, kableExtra, wooldridge, hrbrthemes, scales, ggeasy, patchwork)




# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F,
  dpi=300
)

theme_set(theme_ipsum_rc())

```



# Motivation


---

# Housekeeping

<br><br>


Notes based on `Blitzstein & Hwang (2019)`, ch. 1

  - sections 1.1&#8212;1.3, and 1.6&#8212;1.7

---

# Motivation


<br>

Check out this video:

<br><br>

[`Teach statistics before calculus!, by Arthur Benjamin`](https://www.ted.com/talks/arthur_benjamin_teach_statistics_before_calculus?language=en#t-158820)


---

# Motivation

A quick .hi-blue[quiz]:


Select the statement (*a*, *b*, or *c*) that you agree with most strongly. .hi[There are no wrong answers here!]

<br>

--

How do you interpret the following: "*When flipping a coin, the probability of flipping Heads is 0.5*"?

<br>

  - (*a*) If I flip this coin over and over and over and over, roughly 50% of the flips will be Heads.
  - (*b*) If I flip this coin, Heads / Tails are equally plausible.
  - (*c*) Both of the above make sense.


---

# Motivation

A quick .hi-blue[quiz]:


Select the statement (*a*, *b*, or *c*) that you agree with most strongly. .hi[There are no wrong answers here!]

<br>

--

An election is coming up and a pollster claims that "*candidate A has a 0.9 probability of winning.*" How do you interpret this probability?

<br>

  - (*a*) If we observe the election over and over, candidate A will win roughly 90% of the time.
  - (*b*) Candidate A is much more likely to win than to lose.
  - (*c*) The pollster’s calculation is wrong. Candidate A will either win or lose, thus their probability of winning can only be 0 or 1.
  
  
---

# Motivation

A quick .hi-blue[quiz]:


Select the statement (*a* or *b*) that you agree with most strongly. .hi[There are no wrong answers here!]

--

In a survey with 10 students, they are asked the following 2 questions:

  - Will you vote for a Republican in 2024? 10 out of 10 say "yes."
  - Each student is given a sample of Pepsi and Coke. 10 out of 10 correctly identify which is which.

<br>

In light of these experiments, what do you conclude?

  - (*a*) You’re more confident that students can distinguish between Coke and Pepsi than that students plan to vote for a Republican candidate.
  - (*b*) he evidence in favor of students’ intention to vote a Republican is just as strong as the evidence in favor of  students’ ability to distinguish between Coke and Pepsi.

---

# Motivation

<br><br><br>

If your answers were (*a*), (*a*), and (*b*), you are fairly .hi[frequentist].

--

If your answers were (*b*), (*b*), and (*a*), you are a .hi-blue[Bayesian].

--

If your answers to the first two questions were (*c*) and (*c*), you have .hi-orange[no side!]

---

layout: false
class: inverse, middle

# Studying probability

---

# Studying probability

<br><br>

You have probably either used or heard the words *luck*, *coincidence*, *odds*, *chance* in your life to reflect some .hi[guess] or .hi-blue[prediction] about future events.

--

These can also be understood as .hi[probability statements].

--

  - Perhaps you don't usually .hi-green[quantify] these guesses!


--

Whether you translate such predictions into numbers or not, in the next lectures we will study probability as a .hi-blue[logical path] to measure *uncertainty* and *randomness* through *theoretical principles*.

---

# Studying probability

Probability is the .hi[logic of uncertainty]. 

--

It quantifies uncertainty in a way through which we may 

  - formalize such uncertainty;
  
  - make informed decisions about uncertain events;
  
  - make inferences about noisy processes and population parameters;
  
  - better understand our surroundings.
  
---
layout: false
class: inverse, middle

# Basics of set theory

---

# Basics of set theory

The mathematical framework for probability is built around .hi[sets].

--

Suppose we run an .red[*experiment*], and its result is one out of a set of possible .red[*outcomes*] (.red[*events*]).

--

Before one can quantify the inherent *randomness* in this experiment, one must understand its possible outcomes.

--

Consider some definitions:

  - A .hi-slate[sample space] (*S*) is the collection of all possible outcomes of an experiment;
  
  - An .hi-slate[event] is an element or collection of elements in the sample space, typically denoted by capital letters (e.g., *A, B, C*).
  
    - Thus an event is a *subset* of the sample space *S*: *A* &#8838; *S*.
    
---

# Basics of set theory



Some .hi-blue[special events]:


- Empty (null) set (&#8709;): contains .hi[no] outcomes.

--

- The complement of an event (*A*<sup>*C*</sup>): contains .hi-slate[all] outcomes that are not in event *A*.

--

- Intersection between events (*A* &#8745; *B*): contains all outcomes that are in .hi[both] *A* and *B*.

  - *A* and *B* are .hi-blue[disjoint] if *A* &#8745; *B* = &#8709;.
  
--
  
- Union between events (*A* &#8746; *B*): contains all outcomes that are in *A* .hi-slate[or] *B* .hi-slate[or] both.

--

- De Morgan's laws: 
  
  - (*A* &#8746; *B*)<sup>*C*</sup> = *A*<sup>*C*</sup> &#8745; *B*<sup>*C*</sup>
  - (*A* &#8745; *B*)<sup>*C*</sup> = *A*<sup>*C*</sup> &#8746; *B*<sup>*C*</sup>

---

layout: false
class: inverse, middle

# Many shades of probability

---

# Many shades of probability

Now it is time to study the .hi[uncertainty] associated with the possible outcomes of an experiment.

--

To this purpose, we will need .hi-blue[probability].

--

Informally, one may define probability as *a measure of uncertainty, ranging between 0 (impossible outcome) and 1 (almost certain event).*

--

<br>

But there are more .hi-slate[formal] ways of defining it:

  - The "*naive*" definition of probability;
  
  - The "*frequency*" definition of probability;
  
  - The "*Bayesian*" definition of probability.


---

# Many shades of probability

The .hi[earliest] definition of the probability of an event was to .hi-slate[count] the number of *ways* the event could happen and divide by the *total* number of possible outcomes for the experiment.

$$
\begin{aligned}
P^{naive}(A) = \dfrac{\text{number of ways A can happen}}{\text{total number of outcomes in} \ S}
\end{aligned}
$$

--

<br>

The naive definition is very .hi[restrictive]. 

  - It requires *S* to be *finite*, with equal symmetry across events/outcomes.
  
  - Events must be *equally likely*!
  
---

# Many shades of probability

Given the .hi-blue[limitations] of the naive definition of probability, we will move on to more general and effective concepts.


> A *probability space* consists of a sample space *S* and a *probability function* *P* which takes an event *A* &#8838; *S* as input and returns *P(A)*, a real number between 0 and 1, as output.

--

The function *P(&middot;)* must satisfy the following .hi[two] axioms:

  - *P*(&#8709;) = 0;
  - *P(S)* = 1
  
--

If events *A*<sub>1</sub>, *A*<sub>2</sub>, *A*<sub>3</sub>,... are .hi-blue[disjoint], then

$$
\begin{aligned}
P \bigg(\bigcup_{j=1}^{\infty} A_j \bigg) = \sum_{j=1}^{\infty} P(A_j)
\end{aligned}
$$

---

# Many shades of probability


Any function *P(&middot;)* that satisfies the two axioms is considered a .hi[valid] probability function.

--

However, the axioms don’t tell us how probability should be .hi-blue[interpreted].

--

There are different .hi-slate[schools of thought] regarding such interpretation.

--

<br>

The .hi[frequentist] view of probability is that it represents a long-run frequency over a large number of repetitions of an experiment: if we say a coin has probability 1/2 of Heads, that means the coin would land Heads 50% of the time if we tossed it over and over and over.

--

The .hi-slate[Bayesian] view of probability is that it represents a degree of belief about the event in question, so we can assign probabilities to hypotheses like “candidate A will win the election” or “the defendant is guilty” even if it is not possible to repeat the same election or the same crime over and over again.

---

layout: false
class: inverse, middle

# Next time: Properties of probability; conditional probability

---
exclude: true