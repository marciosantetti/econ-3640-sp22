---
title: ".b[Frequentist Inference, pt. II]"
subtitle: ".b[ECON 3640--001]"
author: "Marcio Santetti"
date: "Spring 2022"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'utah-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, ggforce, viridis, dplyr, magrittr, knitr, parallel, xaringanExtra, tidyverse, sjPlot, showtext, mathjaxr, ggforce, furrr, kableExtra, wooldridge, hrbrthemes, scales, ggeasy, patchwork)




# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F,
  dpi=300
)

theme_set(theme_ipsum_rc())

```



# Motivation



---

# Housekeeping

<br><br>


Notes based on `Keller (2009)`:

  - Chapter .b[11], sections `11.1` and `11.2`.
  

  


---

# Motivation

Last time, we were introduced to the concept of .hi[confidence intervals].

--

Given how fragile a .b[point] estimator is, producing inferences about a population parameter through an .b[interval] allows for a more flexible interpretation of a statistic of interest.

--

<br>

Now, we move on to a .hi[second] inferential approach: 

  - .hi-green[Hypothesis testing]
  
--

This procedure serves for determining whether there is .red[*enough statistical evidence*] to confirm a belief/hypothesis about a parameter of interest.

---



layout: false
class: inverse, middle

# A *nonstatistical* application of Hypothesis Testing


---

# A *nonstatistical* application of Hypothesis Testing

<br><br>

When a person is accused of a crime, they face a .hi-green[trial]. 

--

The prosecution presents the case, and a jury must make a decision, .hi-blue[based on the evidence] presented.

--

<br>

In fact, what the jury conducts is a test of .b[different hypotheses]:

--

  - .red[*Prior hypothesis*]: the defendant is .hi-slate[not guilty].
  
  - .red[*Alternative hypothesis*]: the defendant is .hi-slate[guilty].

---

# A *nonstatistical* application of Hypothesis Testing

<br>

The jury .hi[does not] know which hypothesis is correct. 

--

Their base will be the .hi-green[evidence] presented by both prosecution and defense. 

--

In the end, there are only two possible decisions: 

  - .b[Convict] or 
  
  - .b[Acquit].


---

layout: false
class: inverse, middle

# Back to Statistics


---

# Statistical hypothesis testing

The *same reasoning* follows for Statistics:

--

  - The .b[prior hypothesis] is called the .hi[null hypothesis] (*H<sub>0</sub>*);
  
  - The .b[alternative hypothesis] is called the research or .hi[alternative hypothesis] (*H<sub>1</sub>* or *H<sub>a</sub>*).
  
--

Putting the *trial* example in statistical notation:
  
  -  *H<sub>0</sub>*: the defendant is .b[not guilty].
  
  - *H<sub>a</sub>*: the defendant is .b[guilty].
  
--

The hypothesis of the defendant being guilty (*H<sub>a</sub>*) is what we are .hi[actually] testing, since any defendant enters the trial as .red[*innocent*], until proven otherwise.

--

  - That is why this is our .hi-blue[alternative] hypothesis!

---

# Statistical hypothesis testing

If the jury decides to .hi[convict], they are .hi-slate[rejecting] the *null* hypothesis in favor or the *alternative.*

--

In other words, there was .hi-green[enough evidence] to conclude that the defendant was *guilty.*

--

If the jury .hi-blue[acquits], they are .hi-slate[not rejecting] the *null* hypothesis.

--

That is, there was .hi-orange[not enough evidence] to conclude that the defendant was *guilty.*

--

  - .b[Note]: in Statistics, we do not say that we .hi[accept] the null hypothesis. This would mean that the defendant is *innocent*, but we are only able to say that they are not guilty.

---

layout: false
class: inverse, middle


# Type I and Type II errors

---

# Type I and Type II errors

<br>

There are .hi-slate[two] possible errors when working with Hypothesis Testing:

--

  - *Type I error*: when we .hi-blue[reject a true] null hypothesis (*H<sub>0</sub>*);
  
  - *Type II error*: when we .hi-green[do not reject a false] alternative hypothesis (*H<sub>a</sub>*).
  
--

<br>

How do these errors apply in the *trial* context?






---

# Type I and Type II errors

<br>


The .b[probability of a Type I error] is denoted by *&alpha;*, the significance level of a statistical test. 

--

The .b[probability of a Type II error] is denoted by *&beta;*.

--


Any attempt of reducing one probability will increase the odds of the other type of error.

--


<br>



| **Decision**                | **H<sub>0</sub> is true** | **H<sub>0</sub> is false** |
|-----------------------------|---------------------------|----------------------------|
| Reject H<sub>0</sub>        | Type I error              | Correct decision           |
| Do not reject H<sub>0</sub> | Correct decision          | Type II error              |



---

# A quick summary


<br><br>

To summarize, a couple of remarks:

<br>

  - The testing procedure begins with the assumption that the null hypothesis (*H<sub>0</sub>*) is .hi-blue[true];
  
  - The goal is to determine whether there is enough evidence to infer that the alternative hypothesis (*H<sub>a</sub>*) is true.

---


layout: false
class: inverse, middle

# Stating hypotheses

---

# Stating hypotheses

The .hi[first step] when doing hypothesis testing is to .hi-blue[state] the *null* and *alternative* hypotheses, *H<sub>0</sub>* and *H<sub>a</sub>*, respectively.

--

Let us exercise that by practicing with an .b[example].

--

Recall the inventory example from the last lecture.

--

Now, suppose the manager does not want to estimate the exact (or closest) mean inventory level (*&mu;*), but rather test whether this value is .hi[different from] 350 computers. 

--

  - Is there enough evidence to conclude that *&mu;* is .hi-green[not equal to 350 computers]?
  
--

As an important .hi-blue[first note], Hypothesis Testing always tests values for .hi[population parameters].

--

Then, the next step is to know .hi-orange[what] population parameter the problem at hand is referring to.

---

# Stating hypotheses

<br><br><br>

Now, consider the following .hi-slate[change] in the research question for this example:

--

  - Is there enough evidence to conclude that *&mu;* is greater than 350?



---

layout: false
class: inverse, middle

# The *z* test


---

# The *z* test

After the hypotheses are properly stated, what do we do?

--

As a simplfying assumption, we will continue to assume that the population standard deviation (*&sigma;*) is known, while *&mu;* is not.

--

  - We will .hi-green[relax] this hypothesis soon.
  
--

Another example:

A manager is considering establishing a new billing system for customers. After some analyses, they determined that the new system will be cost-effective only if the mean monthly account is more than US$ 170.00. A random sample of 400 monthly accounts is drawn, for which the sample mean is US$ 178.00. The manager assumes that these accounts are normally distributed, with a standard deviation of US$ 65.00. Can the manager conclude from this that the new system will be cost-effective? Also, they assume a confidence level of 95%.




---

# The *z* test

After stating the null and alternative hypotheses, we need to calculate a .hi[test statistic].

--

Recall the .hi-blue[standardization] method for a sample statistic:

$$
\begin{aligned}
z = \dfrac{\bar{x} - \mu}{\sigma / \sqrt{n}}
\end{aligned}
$$

--

<br>

For hypothesis testing purposes, the above is also known as a .hi-slate[*z* test].



---

# The *z* test

After obtaining the *z* value, let us now make use of the confidence level (1 − *&alpha;*) of 95% assumed by the manager.

--

This value will be of use to establish a .hi-slate[threshold (critical) value] in a Standard Normal curve:

```{r, echo=FALSE, dev = "svg", fig.height = 3.5}

min2 <- -3.5
max2 <- 3.5

ggplot(data.frame(x = c(min2, max2)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), geom = "area", fill = "#8b9dc3", alpha = 0.35) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), size = 1.2, alpha = 0.8) +
  scale_x_continuous(labels = comma) +
  geom_area(stat = "function", fun = dnorm, args = list(mean = 0, sd = 1), 
            fill = "#b70673", xlim = c(1.64, 3.5), alpha = 0.4) +
  theme(axis.text.x=element_blank()) +
  labs(x = "z", y = "") +
  easy_x_axis_title_size(13)
```


---

# The *z* test

<br>

The .hi[shaded area] is called the .b[rejection region].

--

If a *z* statistic falls .hi[within] the rejection region, our inference is to .hi-blue[reject the null hypothesis].

--

In case the *z* value falls .hi[outside] this region, then we .hi-blue[do not reject] the null hypothesis.

--

  - So what is our decision from the example?

---

layout: false
class: inverse, middle

# The p-value method


---

# The p-value method

<br>

We may also produce inferences using .hi[p-values] instead of critical values.

--

> The *p-value* of a statistical test is the probability of observing a test statistic .red[*at least as extreme*] as the one which has been computed, .red[given that *H<sub>0</sub>* is true].

--

<br>

  - What is the p-value in our example?

```{r}
1 - pnorm(q = 2.46, mean = 0, sd = 1)
```



---

# The p-value method

A p-value of .0069 implies that there is a .69% probability of observing a sample mean at least as large as US$ 178 when the population mean is US$ 170.

--

In other words, this value says that we have a pretty good sample statistic for our Hypothesis Testing interests.

--

However, such interpretation is .b[almost never used] in practice when considering p-values.

--

<br>

Instead, it is .red[*more convenient*] to compare p-values with the test's significance level (*&alpha;*):

  - If the p-value is .hi-blue[less] than the significance value, we .hi-slate[reject the null hypothesis];
  
  - If the p-value is .hi-blue[greater] than the significance value, we .hi-slate[do not reject the null hypothesis].

---

# The p-value method

Consider, for example, a p-value of .b[.001].

--

This number says that we will only start not to reject the null when the significance level is .hi[lower] than .001.

--

  - Therefore, this means that it would be really .hi-blue[unlikely] not to reject the null hypothesis in such situation.
  
--

We can consider the following .hi-blue[ranges] and .hi-blue[significance] features for p-values:

  - $p < .01$: .b[highly significant] test, *overwhelming* evidence to infer that *H<sub>a</sub>* is true;
  
  - $.01 \leq p \leq .05$: .b[significant] test, *strong* evidence to infer that *H<sub>a</sub>* is true;
  
  - $.05 < p \leq .10$: .b[weakly significant] test;
  
  - $p> .10$: .b[little or no] evidence that *H<sub>a</sub>* is true.


---


layout: false
class: inverse, middle

# One- and two-tailed tests

---

# One- and two-tailed tests

<br>

As you may have already noticed, the typical Normal distribution density curve has two “tails.”

--

Depending on the sign present in the alternative hypothesis (*H<sub>a</sub>*), our test may have .hi[one or two] rejection regions.

--

Whenever the sign in the alternative hypothesis is either “<” or “>,” we have a .b[one-tailed test].

--

  - For the former case, the rejection region lies on the *left* tail of the bell curve, whereas for the latter, the rejection region is located on the *right* tail.
  



---

# One- and two-tailed tests

<br><br>

A two-tailed test will take place whenever the .red[*not equal to*] sign $(\neq)$ is present in the alternative hypothesis.

  - This happens because, assuming this sign, the value of our parameter may lie either on the *right* or on the *left* tail.
  
--

For two-tailed test, we simply divide the significance level (*&alpha;*) by 2.

  - Just as with .hi-green[confidence intervals]!


---

layout: false
class: inverse, middle

# Next time: Inference when *&sigma;* is unknown


---
exclude: true